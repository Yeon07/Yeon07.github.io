<!DOCTYPE html> 
<html lang="en">
   <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <title>Seoyeon Kim</title>
      <meta name="author" content="Seoyeon Kim"/>
      <meta name="description" content="Seoyeon's personal webpage. Based on [*folio](https://github.com/bogoli/-folio) design. "/>
      <meta name="keywords" content="Seoyeon"/>
      <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/>
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
      <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/vs.css" media="none" id="highlight_theme_light"/>
      <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”¥</text></svg>">
      <link rel="stylesheet" href="/assets/css/main.css">
      <link rel="canonical" href="https://yeon07.github.io/SeoyeonKim.github.io/">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/zenburn.css" media="none" id="highlight_theme_dark"/>
      <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> 
   </head>
   <body class="fixed-top-nav ">
      <header>
         <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
            <div class="container">
               <div class="navbar-brand social"> <a href="mailto:syeonkim07@postech.ac.kr" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=s0gtpmIAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/Yeon07" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/seoyeon-kim-91b9bb185/" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> </div>
               <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> 
               <div class="collapse navbar-collapse text-right" id="navbarNav">
                  <ul class="navbar-nav ml-auto flex-nowrap">
                     <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li>
                     <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li>
                     <li class="nav-item"> <a class="nav-link" href="/assets/pdf/seoyeon_cv.pdf" target="_blank" rel="noopener noreferrer">Curriculum Vitae</a> </li>
                     <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li>
                     <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li>
                  </ul>
               </div>
            </div>
         </nav>
      </header>
      <div class="container mt-5">
         <div class="post">
            <header class="post-header">
               <h1 class="post-title"> <span class="font-weight-bold">Seoyeon Kim</span> </h1>
               <p class="desc"><a href="https://cse.postech.ac.kr/" target="_blank" rel="noopener noreferrer">CSE</a> M.Sc. student at <a href="http://cvlab.postech.ac.kr/lab/" target="_blank" rel="noopener noreferrer">Computer Vision Lab.</a>, <a href="https://postech.ac.kr/eng/" target="_blank" rel="noopener noreferrer">POSTECH</a>.</p>
            </header>
            <article>
               <div class="profile float-right">
                  <figure>
                     <picture>
                        <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_namyup-480.webp">
                        </source> 
                        <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_namyup-800.webp">
                        </source> 
                        <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_namyup-1400.webp">
                        </source> <img src="/assets/img/prof_namyup.jpg" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="prof_namyup.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                     </picture>
                  </figure>
                  <div class="address"> </div>
               </div>
               <div class="clearfix">
                  <p>Hi, there! I am a self-motivated Master's Student exploring Computer Vision.</p>
                  <p>My present interests lie in Vision-Language Multimodal tasks, and I am currently working on Referring Image Segmentation under the supervision of <a href="https://jaesik.info/" target="_blank" rel="noopener noreferrer">Jaesik Park</a>. I am also interested in efficiently adapting large foundational models to novel, real-world applications.</p>
                  <p>If you are interested in my work, please feel free to contact me.</p>
               </div>
               <div class="news">
                  <h2>News</h2>
                  <div class="table-responsive">
                     <table class="table table-sm table-borderless">
                        <tr>
                           <th scope="row" style="width:16%">Mar 1, 2023</th>
                           <td> ðŸ“œ A paper on cross-modal retrieval is accepted to CVPR 2023. </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width:16%">Feb 1, 2023</th>
                           <td> ðŸ“œ A paper on domain generalization for semantic segmentation is accepted to ICRA 2023. </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width:16%">Nov 21, 2022</th>
                           <td> ðŸŽ‰ I won the <a href="https://cse.postech.ac.kr/postechian-fellowship-%c2%b7-naver-fellowship-%ec%8b%9c%ec%83%81/?pageds=1&amp;p_id=109&amp;e=&amp;k=&amp;c=&amp;cat=5" target="_blank" rel="noopener noreferrer">NAVER Ph.D. Fellowship 2022</a>. </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width:16%">Nov 7, 2022</th>
                           <td> ðŸŽ‰ I won the <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/winners" target="_blank" rel="noopener noreferrer">Qualcomm Innovation Fellowship 2022</a>. </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width:16%">Jun 27, 2022</th>
                           <td> ðŸ“œ A paper on weakly supervised learning for semantic boundary detection is accepted to IJCV. </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width:16%">Jun 20, 2022</th>
                           <td> ðŸ“œ Two papers on domain generalization and referring image segmentation are accepted to CVPR 2022. </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width:16%">Mar 29, 2022</th>
                           <td> ðŸ“° Our work on referring image segmentation is featured in <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/articles/restr-convolution-free-referring-image-segmentation-using-transformers/" target="_blank" rel="noopener noreferrer">MSRA Highlighted Research</a>. </td>
                        </tr>
                     </table>
                  </div>
               </div>
               <div class="education">
                  <h2>Education</h2>
                  <div class="table-responsive">
                     <table class="table table-sm table-borderless">
                        <tr>
                           <th scope="row">Mar, 2022 - Present</th>
                           <td> <a href="https://postech.ac.kr/eng/" target="_blank" rel="noopener noreferrer">Pohang University of Science and Technology (POSTECH)</a>, Pohang, South Korea <br> Integrated M.S./Ph.D. student in <a href="https://cse.postech.ac.kr/" target="_blank" rel="noopener noreferrer">Computer Science and Engineering</a> <br> Advisor: Prof. <a href="https://suhakwak.github.io/" target="_blank" rel="noopener noreferrer">Suha Kwak</a> </td>
                        </tr>
                        <tr>
                           <th scope="row">Mar, 2011 - Feb, 2018</th>
                           <td> <a href="https://eng.ssu.ac.kr/" target="_blank" rel="noopener noreferrer">Soongsil University</a>, Seoul, South Korea <br> B.S. in Electronic Engineering <br> Advisor: Prof. Dongsung Kim </td>
                        </tr>
                     </table>
                  </div>
               </div>
               <div class="experience">
                  <h2>Experience</h2>
                  <div class="table-responsive">
                     <table class="table table-sm table-borderless">
                        <tr>
                           <th scope="row">Dec, 2020 - Jun, 2021</th>
                           <td>
                              <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" target="_blank" rel="noopener noreferrer">Microsoft Research Asia</a> (Remote), Beijing, China <br> <em>Research Intern</em> 
                              <ul>
                                 <li>Researched on domain generalization and referring image segmentation.</li>
                                 <li>Mentor: Dr. Cuiling Lan</li>
                              </ul>
                           </td>
                        </tr>
                        <tr>
                           <th scope="row">Mar, 2018 - Present</th>
                           <td> <a href="http://cvlab.postech.ac.kr/lab/" target="_blank" rel="noopener noreferrer">Computer Vision Lab</a>, <a href="https://postech.ac.kr/" target="_blank" rel="noopener noreferrer">POSTECH</a>, Pohang, South Korea <br> <em>Research and Teaching Assistant</em> </td>
                        </tr>
                     </table>
                  </div>
               </div>
               <div class="publications">
                  <h2>Publications</h2>
                  <ol class="bibliography">
                     <li>
                        <div class="row">
                           <div class="col-sm-4">
                              <figure>
                                 <picture>
                                    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/kim2023improving-480.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/kim2023improving-800.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/kim2023improving-1400.webp">
                                    </source> <img src="/assets/img/publication_preview/kim2023improving.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                                 </picture>
                              </figure>
                           </div>
                           <div id="kim2023improving" class="col-sm-8">
                              <div class="title">Improving Cross-Modal Retrieval with Set of Diverse Embeddings</div>
                              <div class="author"> Dongwon Kim,  <em>Namyup Kim</em>,  and Suha Kwak </div>
                              <div class="periodical"> <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2023 </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2211.16761" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="http://cvlab.postech.ac.kr/research/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div>
                              <div class="abstract hidden">
                                 <p>Cross-modal retrieval across image and text modalities is a challenging task due to its inherent ambiguity: An image often exhibits various situations, and a caption can be coupled with diverse images. Set-based embedding has been studied as a solution to this problem. It seeks to encode a sample into a set of different embedding vectors that capture different semantics of the sample. In this paper, we present a novel set-based embedding method, which is distinct from previous work in two aspects. First, we present a new similarity function called smooth-Chamfer similarity, which is designed to alleviate the side effects of existing similarity functions for set-based embedding. Second, we propose a novel set prediction module to produce a set of embedding vectors that effectively captures diverse semantics of input by the slot attention mechanism. Our method is evaluated on the COCO and Flickr30K datasets across different visual backbones, where it outperforms existing methods including ones that demand substantially larger computation at inference.</p>
                              </div>
                           </div>
                        </div>
                     </li>
                     <li>
                        <div class="row">
                           <div class="col-sm-4">
                              <figure>
                                 <picture>
                                    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/kim2021wedge-480.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/kim2021wedge-800.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/kim2021wedge-1400.webp">
                                    </source> <img src="/assets/img/publication_preview/kim2021wedge.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                                 </picture>
                              </figure>
                           </div>
                           <div id="kim2021wedge" class="col-sm-8">
                              <div class="title">WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation</div>
                              <div class="author"> <em>Namyup Kim</em>, Taeyoung Son, Jaehyun Pahk, Cuiling Lan, Wenjun Zeng,  and Suha Kwak </div>
                              <div class="periodical"> <em>IEEE International Conference on Robotics and Automation (<b>ICRA</b>),</em> 2023 </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2109.14196" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div>
                              <div class="abstract hidden">
                                 <p>Domain generalization for semantic segmentation is highly demanded in real applications, where a trained model is expected to work well in previously unseen domains. One challenge lies in the lack of data which could cover the diverse distributions of the possible unseen domains for training. In this paper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme, which is the first to exploit the diversity of web-crawled images for generalizable semantic segmentation. To explore and exploit the real-world data distributions, we collect a web-crawled dataset which presents large diversity in terms of weather conditions, sites, lighting, camera styles, etc. We also present a method which injects the style representation of the web-crawled data into the source domain on-the-fly during training, which enables the network to experience images of diverse styles with reliable labels for effective training. Moreover, we use the web-crawled dataset with predicted pseudo labels for training to further enhance the capability of the network. Extensive experiments demonstrate that our method clearly outperforms existing domain generalization techniques.</p>
                              </div>
                           </div>
                        </div>
                     </li>
                     <li>
                        <div class="row">
                           <div class="col-sm-4">
                              <figure>
                                 <picture>
                                    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/kim2022restr-480.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/kim2022restr-800.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/kim2022restr-1400.webp">
                                    </source> <img src="/assets/img/publication_preview/kim2022restr.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                                 </picture>
                              </figure>
                           </div>
                           <div id="kim2022restr" class="col-sm-8">
                              <div class="title">ReSTR: Convolution-free Referring Image Segmentation Using Transformers</div>
                              <div class="author"> <em>Namyup Kim</em>, Dongwon Kim, Cuiling Lan, Wenjun Zeng,  and Suha Kwak </div>
                              <div class="periodical"> <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2022 </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2203.16768" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="http://cvlab.postech.ac.kr/research/restr/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div>
                              <div class="abstract hidden">
                                 <p>Referring image segmentation is an advanced semantic segmentation task where target is not a predefined class but is described in natural language. Most of existing methods for this task rely heavily on convolutional neural networks, which however have trouble capturing long-range dependencies between entities in the language expression and are not flexible enough for modeling interactions between the two different modalities. To address these issues, we present the first convolution-free model for referring image segmentation using transformers, dubbed ReSTR. Since it extracts features of both modalities through transformer encoders, it can capture long-range dependencies between entities within each modality. Also, ReSTR fuses features of the two modalities by a self-attention encoder, which enables flexible and adaptive interactions between the two modalities in the fusion process. The fused features are fed to a segmentation module, which works adaptively according to the image and language expression in hand. ReSTR is evaluated and compared with previous work on all public benchmarks, where it outperforms all existing models.</p>
                              </div>
                           </div>
                        </div>
                     </li>
                     <li>
                        <div class="row">
                           <div class="col-sm-4">
                              <figure>
                                 <picture>
                                    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/kang2022styneophile-480.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/kang2022styneophile-800.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/kang2022styneophile-1400.webp">
                                    </source> <img src="/assets/img/publication_preview/kang2022styneophile.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                                 </picture>
                              </figure>
                           </div>
                           <div id="kang2022styneophile" class="col-sm-8">
                              <div class="title">Style Neophile: Constantly Seeking Novel Styles for Domain Generalization</div>
                              <div class="author"> Juwon Kang, Sohyun Lee,  <em>Namyup Kim</em>,  and Suha Kwak </div>
                              <div class="periodical"> <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2022 </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://cvlab.postech.ac.kr/research/StyleNeophile/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div>
                              <div class="abstract hidden">
                                 <p>This paper studies domain generalization via domain-invariant representation learning. Existing methods in this direction suppose that a domain can be characterized by styles of its images, and train a network using style-augmented data so that the network is not biased to par-ticular style distributions. However, these methods are restricted to a finite set of styles since they obtain styles for augmentation from a fixed set of external images or by interpolating those of training data. To address this limitation and maximize the benefit of style augmentation, we propose a new method that synthesizes novel styles constantly during training. Our method manages multiple queues to store styles that have been observed so far, and synthesizes novel styles whose distribution is distinct from the distribution of styles in the queues. The style synthesis process is formulated as a monotone submodular optimization, thus can be conducted efficiently by a greedy algorithm. Extensive experiments on four public benchmarks demonstrate that the proposed method is capable of achieving state-of-the-art domain generalization performance.</p>
                              </div>
                           </div>
                        </div>
                     </li>
                     <li>
                        <div class="row">
                           <div class="col-sm-4">
                              <figure>
                                 <picture>
                                    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/kim2022learning-480.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/kim2022learning-800.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/kim2022learning-1400.webp">
                                    </source> <img src="/assets/img/publication_preview/kim2022learning.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                                 </picture>
                              </figure>
                           </div>
                           <div id="kim2022learning" class="col-sm-8">
                              <div class="title">Learning to Detect Semantic Boundaries with Image-Level Class Labels</div>
                              <div class="author"> <em>Namyup Kim*</em>, Sehyun Hwang*,  and Suha Kwak (*equal contribution) </div>
                              <div class="periodical"> <em>International Journal of Computer Vision (<b>IJCV</b>),</em> 2022 </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2212.07579" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div>
                              <div class="abstract hidden">
                                 <p>This paper presents the first attempt to learn semantic boundary detection using image-level class labels as supervision. Our method starts by estimating coarse areas of object classes through attentions drawn by an image classification network. Since boundaries will locate somewhere between such areas of different classes, our task is formulated as a multiple instance learning (MIL) problem, where pixels on a line segment connecting areas of two different classes are regarded as a bag of boundary candidates. Moreover, we design a new neural network architecture that can learn to estimate semantic boundaries reliably even with uncertain supervision given by the MIL strategy. Our network is used to generate pseudo semantic boundary labels of training images, which are in turn used to train fully supervised models. The final model trained with our pseudo labels achieves an outstanding performance on the SBD dataset, where it is as competitive as some of previous arts trained with stronger supervision.</p>
                              </div>
                           </div>
                        </div>
                     </li>
                     <li>
                        <div class="row">
                           <div class="col-sm-4">
                              <figure>
                                 <picture>
                                    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/son2020urie-480.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/son2020urie-800.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/son2020urie-1400.webp">
                                    </source> <img src="/assets/img/publication_preview/son2020urie.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                                 </picture>
                              </figure>
                           </div>
                           <div id="son2020urie" class="col-sm-8">
                              <div class="title">Urie: Universal image enhancement for visual recognition in the wild</div>
                              <div class="author"> Taeyoung Son, Juwon Kang,  <em>Namyup Kim</em>, Sunghyun Cho,  and Suha Kwak </div>
                              <div class="periodical"> <em>European Conference on Computer Vision (<b>ECCV</b>),</em> 2020 </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2007.08979" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://github.com/taeyoungson/urie" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="http://cvlab.postech.ac.kr/research/URIE/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div>
                              <div class="abstract hidden">
                                 <p>Despite the great advances in visual recognition, it has been witnessed that recognition models trained on clean images of common datasets are not robust against distorted images in the real world. To tackle this issue, we present a Universal and Recognition-friendly Image Enhancement network, dubbed URIE, which is attached in front of existing recognition models and enhances distorted input to improve their performance without retraining them. URIE is universal in that it aims to handle various factors of image degradation and to be incorporated with any arbitrary recognition models. Also, it is recognition-friendly since it is optimized to improve the robustness of following recognition models, instead of perceptual quality of output image. Our experiments demonstrate that URIE can handle various and latent image distortions and improve the performance of existing models for five diverse recognition tasks when input images are degraded.</p>
                              </div>
                           </div>
                        </div>
                     </li>
                  </ol>
               </div>
               <div class="honors">
                  <h2>Honors and Awards</h2>
                  <div class="table-responsive">
                     <table class="table table-sm table-borderless">
                        <tr>
                           <td>
                              <strong>POSTECHIAN Fellowship Award (2022)</strong> 
                              <ul>
                                 <li>Winner ($5,000)</li>
                              </ul>
                              <strong>NAVER Ph.D. Fellowship Award (2022)</strong> 
                              <ul>
                                 <li>Winner ($4,000)</li>
                              </ul>
                              <strong>Qualcomm Innovation Fellowship South Korea (2022)</strong> 
                              <ul>
                                 <li>Winner ($3,000) - <em>ReSTR: Convolutionâ€‘free Referring Image Segmentation Using Transformers</em> (CVPR2022)</li>
                              </ul>
                              <strong>NAVER \(\times\) POSTECH AI DAY (2022)</strong> 
                              <ul>
                                 <li>The 2\(^{\textnormal{nd}}\) and 3\(^{\textnormal{rd}}\) Prize - <em>ReSTR: Convolutionâ€‘free Referring Image Segmentation Using Transformers</em> (CVPR2022)</li>
                              </ul>
                              <strong>The 26th HumanTech Paper Award, Samsung Electronics Co., Ltd. (2020)</strong> 
                              <ul>
                                 <li>The Honorable Mention ($3,000) - <em>Learning to Detect Semantic Boundaries with Imageâ€‘Level Class Labels</em> (IJCV2022)</li>
                              </ul>
                           </td>
                        </tr>
                     </table>
                  </div>
               </div>
               <div class="social">
                  <div class="contact-icons"> <a href="mailto:%6E%61%6D%79%75%70@%70%6F%73%74%65%63%68.%61%63.%6B%72" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=s0gtpmIAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/southflame" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/namyupkim" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> </div>
                  <div class="contact-note"> </div>
               </div>
            </article>
         </div>
      </div>
      <footer class="fixed-bottom">
         <div class="container mt-0"> Â© Copyright 2023 Namyup Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div>
      </footer>
      <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> 
   </body>
</html>
