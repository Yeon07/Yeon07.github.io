<!DOCTYPE html> 
<html lang="en">
   <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <title>Seoyeon Kim</title>
      <meta name="author" content="Seoyeon Kim"/>
      <meta name="description" content="Seoyeon's personal webpage. Based on [*folio](https://github.com/bogoli/-folio) design. "/>
      <meta name="keywords" content="Seoyeon"/>
      <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/>
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
      <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/vs.css" media="none" id="highlight_theme_light"/>
      <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🔥</text></svg>">
      <link rel="stylesheet" href="/assets/css/main.css">
      <link rel="canonical" href="https://yeon07.github.io/SeoyeonKim.github.io/">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/zenburn.css" media="none" id="highlight_theme_dark"/>
      <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> 
   </head>
   <body class="fixed-top-nav ">
      <header>
         <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
            <div class="container">
               <div class="navbar-brand social"> <a href="mailto:syeonkim07@postech.ac.kr" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?hl=en&user=-SnIx4kAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/Yeon07" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/seoyeon-kim-91b9bb185/" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> </div>
               <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> 
               <div class="collapse navbar-collapse text-right" id="navbarNav">
                  <ul class="navbar-nav ml-auto flex-nowrap">
                     <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li>
                     <!--<li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li>-->
                     <li class="nav-item"> <a class="nav-link" href="/assets/pdf/cv_seoyeon.pdf" target="_blank" rel="noopener noreferrer">Curriculum Vitae</a> </li>
                     <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li>
                     <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li>
                  </ul>
               </div>
            </div>
         </nav>
      </header>
      <div class="container mt-5">
         <div class="post">
            <header class="post-header">
               <h1 class="post-title"> <span class="font-weight-bold">Seoyeon Kim</span> </h1>
               <p class="desc"><strong>M.S. student at <a href="http://cvlab.postech.ac.kr/lab/" target="_blank" rel="noopener noreferrer">Computer Vision Lab</a>, <a href="https://postech.ac.kr/eng/" target="_blank" rel="noopener noreferrer">POSTECH</a>.</strong></p>
            </header>
            <article>
               <div class="profile float-right"> 
                  <figure>
                     <picture>
                        <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_seoyeon-480.webp">
                        </source> 
                        <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_seoyeon-800.webp">
                        </source> 
                        <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_seoyeon-1400.webp">
                        </source> <img src="/assets/img/prof_seoyeon.jpg" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="prof_seoyeon.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                     </picture>
                  </figure>
                  <div class="address"> </div>
               </div>
               <div class="clearfix">
                  <p>Hi, there! I am a self-motivated Master's Student exploring Computer Vision and Deep Learning.</p>
                  <p>I am enrolled to the <a href="https://cse.postech.ac.kr/" target="_blank" rel="noopener noreferrer">Computer Science and Engineering</a> department at <a href="https://postech.ac.kr/eng/" target="_blank" rel="noopener noreferrer">POSTECH</a>, working as a member of <a href="http://cvlab.postech.ac.kr/lab/" target="_blank" rel="noopener noreferrer">Computer Vision Lab.</a> under the supervision of <a href="https://suhakwak.github.io/" target="_blank" rel="noopener noreferrer">Prof. Suha Kwak</a>.</p>
                  <p>My present interests lie in Vision-Language Multimodal tasks, and I recenly published a work on Referring Image Segmentation. I am specifically interested in adapting large foundational models for novel, real-world applications.</p>
                  <p>If you are interested in my work, feel free to <a href="mailto:syeonkim07@postech.ac.kr">contact me</a>.</p>
               </div>
               <!--
               <div class="news">
                  <h2>News</h2>
                  <div class="table-responsive">
                     <table class="table table-sm table-borderless">
                        <tr>
                           <th scope="row" style="width:16%">Mar 1, 2023</th>
                           <td> 📜 A paper on cross-modal retrieval is accepted to CVPR 2023. </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width:16%">Feb 1, 2023</th>
                           <td> 📜 A paper on domain generalization for semantic segmentation is accepted to ICRA 2023. </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width:16%">Nov 21, 2022</th>
                           <td> 🎉 I won the <a href="https://cse.postech.ac.kr/postechian-fellowship-%c2%b7-naver-fellowship-%ec%8b%9c%ec%83%81/?pageds=1&amp;p_id=109&amp;e=&amp;k=&amp;c=&amp;cat=5" target="_blank" rel="noopener noreferrer">NAVER Ph.D. Fellowship 2022</a>. </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width:16%">Nov 7, 2022</th>
                           <td> 🎉 I won the <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/winners" target="_blank" rel="noopener noreferrer">Qualcomm Innovation Fellowship 2022</a>. </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width:16%">Jun 27, 2022</th>
                           <td> 📜 A paper on weakly supervised learning for semantic boundary detection is accepted to IJCV. </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width:16%">Jun 20, 2022</th>
                           <td> 📜 Two papers on domain generalization and referring image segmentation are accepted to CVPR 2022. </td>
                        </tr>
                        <tr>
                           <th scope="row" style="width:16%">Mar 29, 2022</th>
                           <td> 📰 Our work on referring image segmentation is featured in <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/articles/restr-convolution-free-referring-image-segmentation-using-transformers/" target="_blank" rel="noopener noreferrer">MSRA Highlighted Research</a>. </td>
                        </tr>
                     </table>
                  </div>
               </div>
               --> 
               <div class="education">
                  <h2>Education</h2>
                  <div class="table-responsive">
                     <table class="table table-sm table-borderless">
                        <tr>
                           <th scope="row">Sept, 2022 - Present</th>
                           <td> <a href="https://postech.ac.kr/eng/" target="_blank" rel="noopener noreferrer">POSTECH</a>, Pohang, South Korea <br> M.S. student in <a href="https://cse.postech.ac.kr/" target="_blank" rel="noopener noreferrer">Computer Science and Engineering</a> <br> <u>GPA 4.15/4.30</u> <br> Advised by <a href="https://suhakwak.github.io/" target="_blank" rel="noopener noreferrer">Prof. Suha Kwak</a> </td>
                        </tr>
                        <tr>
                           <th scope="row">Feb, 2018 - Aug, 2022</th>
                           <td> <a href="https://postech.ac.kr/eng/" target="_blank" rel="noopener noreferrer">POSTECH</a>, Pohang, South Korea <br> B.S. in <a href="https://cse.postech.ac.kr/" target="_blank" rel="noopener noreferrer">Computer Science and Engineering</a> <br> <u>Summa Cum Laude</u> <br> Advised by <a href="https://jaesik.info/" target="_blank" rel="noopener noreferrer">Prof. Jaesik Park</a> </td>
                        </tr>
                        <tr>
                           <th scope="row">Sept, 2013 - Aug, 2017</th>
                           <td> <a href="https://www.freemens.org/" target="_blank" rel="noopener noreferrer">City of London Freemen's School</a>, Surrey, United Kingdom <br> <u>4 A*s in A-level</u> (Maths, Further Maths, Physics, Chemistry) </td>
                        </tr>
                     </table>
                  </div>
               </div>
               <div class="experience">
                  <h2>Experience</h2>
                  <div class="table-responsive">
                     <table class="table table-sm table-borderless">
                        <tr>
                           <th scope="row">June, 2021 - Sept, 2021</th>
                           <td>
                              <a href="https://www.hyundaimotorgroup.com/main/mainRecommend" target="_blank" rel="noopener noreferrer">Hyundai Motor Group</a>, Seoul, South Korea <br> <em>Research Intern</em> 
                              <ul>
                                 <li>Researched Object Detection and Neural Style Transfer.</li>
                              </ul>
                           </td>
                        </tr>
                        <tr>
                           <th scope="row">Feb, 2021 - Aug, 2022 </th>
                           <td> 
                              <a href="http://cvlab.postech.ac.kr/lab/" target="_blank" rel="noopener noreferrer">Computer Vision Lab</a>, <a href="https://postech.ac.kr/" target="_blank" rel="noopener noreferrer">POSTECH</a>, Pohang, South Korea <br> <em>Undergraduate Research Intern</em>
                              <ul>
                                 <li>Developed Sample-Dependent Occlusion Network for Intelligent Data Augmentation.</li>
                                 <li>Proposed Patch-wise Matching Framework for Self-Supervised Learning in Vision Transformers.</li>
                              </ul>
                           </td>
                        </tr>
                     </table>
                  </div>
               </div>
               <div class="publications">
                  <h2>Publications</h2>
                  <ol class="bibliography">
                     <li>
                        <div class="row">
                           <div class="col-sm-4">
                              <figure>
                                 <picture>
                                    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/kim2023risclipv2-480.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/kim2023risclipv2-800.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/kim2023risclipv2-1400.webp">
                                    </source> <img src="/assets/img/publication_preview/kim2023risclipv2.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                                 </picture>
                              </figure>
                           </div>
                           <div id="kim2023risclip" class="col-sm-8">
                              <div class="title">RISCLIP: Referring Image Segmentation Framework using CLIP</div>
                              <div class="author"> <b>Seoyeon Kim</b>,  Minguk Kang,  and Jaesik Park </div>
                              <div class="periodical"> <em>Under Submission,</em> 2023 </div>
                              <!-- <div class="periodical"> <em>International Conference on Neural Information Processing Systems (<b>NeurlPS</b>),</em> 2023 </div> -->
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://arxiv.org/abs/2306.08498" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://github.com/Yeon07/RISCLIP" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div>
                              <div class="abstract hidden">
                                 <p>Recent advances in computer vision and natural language processing have naturally led to active research in multi-modal tasks, including Referring Image Segmentation (RIS). Recent approaches have advanced the frontier of RIS by impressive margins, but they require an additional pretraining stage on external visual grounding datasets to achieve the state-of-the-art performances. We attempt to break free from this requirement by effectively adapting Contrastive Language-Image Pretraining (CLIP) to RIS. We propose a novel framework that residually adapts frozen CLIP features to RIS with Fusion Adapters and Backbone Adapters. Freezing CLIP preserves the backbone's rich, general image-text alignment knowledge, whilst Fusion Adapters introduce multi-modal communication and Backbone Adapters inject new knowledge useful in solving RIS. Our method reaches a new state of the art on three major RIS benchmarks. We attain such performance without additional pretraining and thereby absolve the necessity of extra training and data preparation.</p>
                              </div>
                           </div>
                        </div>
                     </li>
                     <li>
                        <div class="row">
                           <div class="col-sm-4">
                              <figure>
                                 <picture>
                                    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/kim2023pam-480.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/kim2023pam-800.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/kim2023pam-1400.webp">
                                    </source> <img src="/assets/img/publication_preview/kim2023pam.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                                 </picture>
                              </figure>
                           </div>
                           <div id="kim2023patch" class="col-sm-8">
                              <div class="title">PAM: Patch Aware Matching for Vision Transformer based Self-Supervised Learning Frameworks</div>
                              <div class="author"> <b>Seoyeon Kim</b>,  Minguk Kang,  and Jaesik Park </div>
                              <div class="periodical"> <em>Image Processing and Image Understanding (IPIU),</em> Bronze Award, 2023 </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> </div>
                              <div class="abstract hidden">
                                 <p>Self-supervised Learning (SSL) is a field that learns meaningful representations from large unlabeled datasets and finetunes such representations to target downstream tasks. Recently, many SSL methods that employ the Vision Transformer based Teacher-Student framework with different losses have been proven effective—even effective enough to outperform their supervised counterparts trained with labeled data. In line with this effective ViT-based Teacher-Student framework, we propose a new loss named “Patch Aware Matching (PAM)” which performs patch token feature distillation across the local view features output from the student and global view features output from the teacher. We hypothesize that such an approach learns “local-to-global” correspondence along with local, structural information on the patch level. When trained and evaluated under the k-NN protocol with a ViT-Small/16 backbone, our approach outperforms state-of-the-art methods on the 100 easiest classes of ImageNet-1K but falls behind on 10% of ImageNet. Also, we come across interesting results that suggest that easier classes of ImageNet-1K are more sample efficient. As a work in progress, we aim to further develop our method and investigate sample efficiency of different ImageNet-1K classes.</p>
                              </div>
                           </div>
                        </div>
                     </li>
                     <!--
                     <li>
                        <div class="row">
                           <div class="col-sm-4">
                              <figure>
                                 <picture>
                                    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/kim2021wedge-480.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/kim2021wedge-800.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/kim2021wedge-1400.webp">
                                    </source> <img src="/assets/img/publication_preview/kim2021wedge.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                                 </picture>
                              </figure>
                           </div>
                           <div id="kim2021wedge" class="col-sm-8">
                              <div class="title">WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation</div>
                              <div class="author"> <em>Seoyeon Kim</em>, Taeyoung Son, Jaehyun Pahk, Cuiling Lan, Wenjun Zeng,  and Suha Kwak </div>
                              <div class="periodical"> <em>IEEE International Conference on Robotics and Automation (<b>ICRA</b>),</em> 2023 </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2109.14196" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div>
                              <div class="abstract hidden">
                                 <p>Domain generalization for semantic segmentation is highly demanded in real applications, where a trained model is expected to work well in previously unseen domains. One challenge lies in the lack of data which could cover the diverse distributions of the possible unseen domains for training. In this paper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme, which is the first to exploit the diversity of web-crawled images for generalizable semantic segmentation. To explore and exploit the real-world data distributions, we collect a web-crawled dataset which presents large diversity in terms of weather conditions, sites, lighting, camera styles, etc. We also present a method which injects the style representation of the web-crawled data into the source domain on-the-fly during training, which enables the network to experience images of diverse styles with reliable labels for effective training. Moreover, we use the web-crawled dataset with predicted pseudo labels for training to further enhance the capability of the network. Extensive experiments demonstrate that our method clearly outperforms existing domain generalization techniques.</p>
                              </div>
                           </div>
                        </div>
                     </li>
                     <li>
                        <div class="row">
                           <div class="col-sm-4">
                              <figure>
                                 <picture>
                                    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/kim2022restr-480.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/kim2022restr-800.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/kim2022restr-1400.webp">
                                    </source> <img src="/assets/img/publication_preview/kim2022restr.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                                 </picture>
                              </figure>
                           </div>
                           <div id="kim2022restr" class="col-sm-8">
                              <div class="title">ReSTR: Convolution-free Referring Image Segmentation Using Transformers</div>
                              <div class="author"> <em>Seoyeon Kim</em>, Dongwon Kim, Cuiling Lan, Wenjun Zeng,  and Suha Kwak </div>
                              <div class="periodical"> <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2022 </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2203.16768" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="http://cvlab.postech.ac.kr/research/restr/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div>
                              <div class="abstract hidden">
                                 <p>Referring image segmentation is an advanced semantic segmentation task where target is not a predefined class but is described in natural language. Most of existing methods for this task rely heavily on convolutional neural networks, which however have trouble capturing long-range dependencies between entities in the language expression and are not flexible enough for modeling interactions between the two different modalities. To address these issues, we present the first convolution-free model for referring image segmentation using transformers, dubbed ReSTR. Since it extracts features of both modalities through transformer encoders, it can capture long-range dependencies between entities within each modality. Also, ReSTR fuses features of the two modalities by a self-attention encoder, which enables flexible and adaptive interactions between the two modalities in the fusion process. The fused features are fed to a segmentation module, which works adaptively according to the image and language expression in hand. ReSTR is evaluated and compared with previous work on all public benchmarks, where it outperforms all existing models.</p>
                              </div>
                           </div>
                        </div>
                     </li>
                     <li>
                        <div class="row">
                           <div class="col-sm-4">
                              <figure>
                                 <picture>
                                    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/kang2022styneophile-480.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/kang2022styneophile-800.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/kang2022styneophile-1400.webp">
                                    </source> <img src="/assets/img/publication_preview/kang2022styneophile.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                                 </picture>
                              </figure>
                           </div>
                           <div id="kang2022styneophile" class="col-sm-8">
                              <div class="title">Style Neophile: Constantly Seeking Novel Styles for Domain Generalization</div>
                              <div class="author"> Juwon Kang, Sohyun Lee,  <em>Seoyeon Kim</em>,  and Suha Kwak </div>
                              <div class="periodical"> <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),</em> 2022 </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://cvlab.postech.ac.kr/research/StyleNeophile/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div>
                              <div class="abstract hidden">
                                 <p>This paper studies domain generalization via domain-invariant representation learning. Existing methods in this direction suppose that a domain can be characterized by styles of its images, and train a network using style-augmented data so that the network is not biased to par-ticular style distributions. However, these methods are restricted to a finite set of styles since they obtain styles for augmentation from a fixed set of external images or by interpolating those of training data. To address this limitation and maximize the benefit of style augmentation, we propose a new method that synthesizes novel styles constantly during training. Our method manages multiple queues to store styles that have been observed so far, and synthesizes novel styles whose distribution is distinct from the distribution of styles in the queues. The style synthesis process is formulated as a monotone submodular optimization, thus can be conducted efficiently by a greedy algorithm. Extensive experiments on four public benchmarks demonstrate that the proposed method is capable of achieving state-of-the-art domain generalization performance.</p>
                              </div>
                           </div>
                        </div>
                     </li>
                     <li>
                        <div class="row">
                           <div class="col-sm-4">
                              <figure>
                                 <picture>
                                    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/kim2022learning-480.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/kim2022learning-800.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/kim2022learning-1400.webp">
                                    </source> <img src="/assets/img/publication_preview/kim2022learning.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                                 </picture>
                              </figure>
                           </div>
                           <div id="kim2022learning" class="col-sm-8">
                              <div class="title">Learning to Detect Semantic Boundaries with Image-Level Class Labels</div>
                              <div class="author"> <em>Seoyeon Kim*</em>, Sehyun Hwang*,  and Suha Kwak (*equal contribution) </div>
                              <div class="periodical"> <em>International Journal of Computer Vision (<b>IJCV</b>),</em> 2022 </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2212.07579" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div>
                              <div class="abstract hidden">
                                 <p>This paper presents the first attempt to learn semantic boundary detection using image-level class labels as supervision. Our method starts by estimating coarse areas of object classes through attentions drawn by an image classification network. Since boundaries will locate somewhere between such areas of different classes, our task is formulated as a multiple instance learning (MIL) problem, where pixels on a line segment connecting areas of two different classes are regarded as a bag of boundary candidates. Moreover, we design a new neural network architecture that can learn to estimate semantic boundaries reliably even with uncertain supervision given by the MIL strategy. Our network is used to generate pseudo semantic boundary labels of training images, which are in turn used to train fully supervised models. The final model trained with our pseudo labels achieves an outstanding performance on the SBD dataset, where it is as competitive as some of previous arts trained with stronger supervision.</p>
                              </div>
                           </div>
                        </div>
                     </li>
                     <li>
                        <div class="row">
                           <div class="col-sm-4">
                              <figure>
                                 <picture>
                                    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/son2020urie-480.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/son2020urie-800.webp">
                                    </source> 
                                    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/son2020urie-1400.webp">
                                    </source> <img src="/assets/img/publication_preview/son2020urie.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> 
                                 </picture>
                              </figure>
                           </div>
                           <div id="son2020urie" class="col-sm-8">
                              <div class="title">Urie: Universal image enhancement for visual recognition in the wild</div>
                              <div class="author"> Taeyoung Son, Juwon Kang,  <em>Seoyeon Kim</em>, Sunghyun Cho,  and Suha Kwak </div>
                              <div class="periodical"> <em>European Conference on Computer Vision (<b>ECCV</b>),</em> 2020 </div>
                              <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2007.08979" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://github.com/taeyoungson/urie" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="http://cvlab.postech.ac.kr/research/URIE/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div>
                              <div class="abstract hidden">
                                 <p>Despite the great advances in visual recognition, it has been witnessed that recognition models trained on clean images of common datasets are not robust against distorted images in the real world. To tackle this issue, we present a Universal and Recognition-friendly Image Enhancement network, dubbed URIE, which is attached in front of existing recognition models and enhances distorted input to improve their performance without retraining them. URIE is universal in that it aims to handle various factors of image degradation and to be incorporated with any arbitrary recognition models. Also, it is recognition-friendly since it is optimized to improve the robustness of following recognition models, instead of perceptual quality of output image. Our experiments demonstrate that URIE can handle various and latent image distortions and improve the performance of existing models for five diverse recognition tasks when input images are degraded.</p>
                              </div>
                           </div>
                        </div>
                     </li>
                     -->
                  </ol>
               </div>
               <div class="honors">
                  <h2>Awards and Scholarships</h2>
                  <div class="table-responsive">
                     <table class="table table-sm table-borderless">
                        <tr>
                           <td>
                              <b>First Place in Research Project I Course, POSTECH (2021)</b> 
                              <ul>
                                 <li>Won Best Award for outstanding research effort</li>
                              </ul>
                              <b>National Science and Engineering Scholarship, Korea Student Aid Foundation (2022)</b> 
                              <ul>
                                 <li>Received two years full scholarhip for academic excellence </li>
                              </ul>
                              <b>Global Leadership Program, POSTECH (2020, 2021)</b> 
                              <ul>
                                 <li>Received CSE scholarship twice for academic potential (4K USD) </li>
                              </ul>
                           </td>
                        </tr>
                     </table>
                  </div>
               </div>
               <div class="social">
                  <div class="contact-icons"> <a href="mailto:syeonkim07@postech.ac.kr" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?hl=en&user=-SnIx4kAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/Yeon07" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/seoyeon-kim-91b9bb185/" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> </div>
                  <div class="contact-note"> </div>
               </div>
            </article>
         </div>
      </div>
      <footer class="fixed-bottom">
         <div class="container mt-0"> © Copyright 2023 Seoyeon Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div>
      </footer>
      <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> 
   </body>
</html>
